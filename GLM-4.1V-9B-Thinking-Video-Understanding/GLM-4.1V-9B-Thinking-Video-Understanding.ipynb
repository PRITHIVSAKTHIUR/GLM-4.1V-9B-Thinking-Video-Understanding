{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***GLM-4.1V-9B-Thinking : Video-Understanding***\n",
        "\n",
        "*notebook by : [prithivMLmods](https://huggingface.co/prithivMLmods)ðŸ¤—*"
      ],
      "metadata": {
        "id": "uFovmijgUV1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Installing all necessary packages***"
      ],
      "metadata": {
        "id": "RugX4SGZV-8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-NtFtjSpuJQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gradio transformers transformers-stream-generator qwen-vl-utils\n",
        "!pip install torchvision torch huggingface_hub numpy\n",
        "!pip install pillow av requests hf_xet spaces\n",
        "#Hold tight, this will take around 3-5 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Run app***"
      ],
      "metadata": {
        "id": "mvoSnRZcVBu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------#\n",
        "#Model used in the app: https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking\n",
        "#\"architectures\": [\"Glm4vForConditionalGeneration\"],\n",
        "#\"model_type\": \"glm4v\",\n",
        "#Change the hardware accelerator to an A100 GPU.\n",
        "#-------------------------------------------------------------------------#\n",
        "import os\n",
        "import time\n",
        "from threading import Thread\n",
        "import gradio as gr\n",
        "import spaces\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from transformers import (\n",
        "    Glm4vForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    TextIteratorStreamer,\n",
        ")\n",
        "\n",
        "# Constants for text generation\n",
        "MAX_MAX_NEW_TOKENS = 3072\n",
        "DEFAULT_MAX_NEW_TOKENS = 2048\n",
        "MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load GLM-4.1V-9B-Thinking\n",
        "MODEL_ID_O = \"THUDM/GLM-4.1V-9B-Thinking\"\n",
        "processor_o = AutoProcessor.from_pretrained(MODEL_ID_O, trust_remote_code=True)\n",
        "model_o = Glm4vForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID_O,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ").to(device).eval()\n",
        "\n",
        "def downsample_video(video_path):\n",
        "    \"\"\"\n",
        "    Downsamples the video to evenly spaced frames.\n",
        "    Each frame is returned as a PIL image along with its timestamp.\n",
        "    \"\"\"\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = []\n",
        "    frame_indices = np.linspace(0, total_frames - 1, 10, dtype=int)\n",
        "    for i in frame_indices:\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        success, image = vidcap.read()\n",
        "        if success:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            pil_image = Image.fromarray(image)\n",
        "            timestamp = round(i / fps, 2)\n",
        "            frames.append((pil_image, timestamp))\n",
        "    vidcap.release()\n",
        "    return frames\n",
        "\n",
        "@spaces.GPU\n",
        "def generate_video(text: str, video_path: str,\n",
        "                   max_new_tokens: int = 1024,\n",
        "                   temperature: float = 0.6,\n",
        "                   top_p: float = 0.9,\n",
        "                   top_k: int = 50,\n",
        "                   repetition_penalty: float = 1.2):\n",
        "    \"\"\"\n",
        "    Generates responses using the GLM-4.1V-9B-Thinking model for video input.\n",
        "    Yields raw text and Markdown-formatted text.\n",
        "    \"\"\"\n",
        "    if video_path is None:\n",
        "        yield \"Please upload a video.\", \"Please upload a video.\"\n",
        "        return\n",
        "\n",
        "    frames = downsample_video(video_path)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text}]}\n",
        "    ]\n",
        "    for frame in frames:\n",
        "        image, timestamp = frame\n",
        "        messages[1][\"content\"].append({\"type\": \"text\", \"text\": f\"Frame {timestamp}:\"})\n",
        "        messages[1][\"content\"].append({\"type\": \"image\", \"image\": image})\n",
        "    inputs = processor_o.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=False,\n",
        "        max_length=MAX_INPUT_TOKEN_LENGTH\n",
        "    ).to(device)\n",
        "    streamer = TextIteratorStreamer(processor_o, skip_prompt=True, skip_special_tokens=True)\n",
        "    generation_kwargs = {\n",
        "        **inputs,\n",
        "        \"streamer\": streamer,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"repetition_penalty\": repetition_penalty,\n",
        "    }\n",
        "    thread = Thread(target=model_o.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "    buffer = \"\"\n",
        "    for new_text in streamer:\n",
        "        buffer += new_text\n",
        "        buffer = buffer.replace(\"<|im_end|>\", \"\")\n",
        "        time.sleep(0.01)\n",
        "        yield buffer, buffer\n",
        "\n",
        "css = \"\"\"\n",
        ".submit-btn {\n",
        "    background-color: #2980b9 !important;\n",
        "    color: white !important;\n",
        "}\n",
        ".submit-btn:hover {\n",
        "    background-color: #3498db !important;\n",
        "}\n",
        ".canvas-output {\n",
        "    border: 2px solid #4682B4;\n",
        "    border-radius: 10px;\n",
        "    padding: 20px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create the Gradio Interface\n",
        "with gr.Blocks(css=css, theme=\"bethecloud/storj_theme\") as demo:\n",
        "    gr.Markdown(\"# GLM-4.1V-9B-Thinking : Video-Understanding\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"Video Inference\"):\n",
        "                    video_query = gr.Textbox(label=\"Query Input\", placeholder=\"Enter your query here...\")\n",
        "                    video_upload = gr.Video(label=\"Video\")\n",
        "                    video_submit = gr.Button(\"Submit\", elem_classes=\"submit-btn\")\n",
        "            with gr.Accordion(\"Advanced options\", open=False):\n",
        "                max_new_tokens = gr.Slider(label=\"Max new tokens\", minimum=1, maximum=MAX_MAX_NEW_TOKENS, step=1, value=DEFAULT_MAX_NEW_TOKENS)\n",
        "                temperature = gr.Slider(label=\"Temperature\", minimum=0.1, maximum=4.0, step=0.1, value=0.6)\n",
        "                top_p = gr.Slider(label=\"Top-p (nucleus sampling)\", minimum=0.05, maximum=1.0, step=0.05, value=0.9)\n",
        "                top_k = gr.Slider(label=\"Top-k\", minimum=1, maximum=1000, step=1, value=50)\n",
        "                repetition_penalty = gr.Slider(label=\"Repetition penalty\", minimum=1.0, maximum=2.0, step=0.05, value=1.2)\n",
        "\n",
        "        with gr.Column():\n",
        "            with gr.Column(elem_classes=\"canvas-output\"):\n",
        "                gr.Markdown(\"## Output\")\n",
        "                output = gr.Textbox(label=\"Raw Output Stream\", interactive=False, lines=2, show_copy_button=True)\n",
        "                with gr.Accordion(\"(Result.md)\", open=False):\n",
        "                    markdown_output = gr.Markdown(label=\"(Result.Md)\")\n",
        "\n",
        "    video_submit.click(\n",
        "        fn=generate_video,\n",
        "        inputs=[video_query, video_upload, max_new_tokens, temperature, top_p, top_k, repetition_penalty],\n",
        "        outputs=[output, markdown_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(max_size=30).launch(share=True, mcp_server=True, ssr_mode=False, show_error=True)"
      ],
      "metadata": {
        "id": "tElKr2Fkp1bO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}